{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# 거리를 계산할 목적\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c573316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB 원본 데이터 읽기\n",
    "fname='./glove.6B/glove.6B.100d.txt'\n",
    "f=open(fname,encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71afa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내용을 살필 목적으로 첫 번째 단어만 출력\n",
    "# for문의 첫 반복에서 break 하므로 첫 요소만 출력함.\n",
    "for line in f: \n",
    "    print(type(line))\n",
    "    print(line)\n",
    "    break\n",
    "# 맨 앞에 the 가 있고 이후에 100개의 실수가 출력됨 ==> the 는 100차원 벡터로 표현되었음    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 구축(딕셔너리 자료구조로 표현)\n",
    "dictionary={}\n",
    "for line in f:\n",
    "    li=line.split()\n",
    "    # 단어에 해당하는 맨 앞 요소를 word 객체에 저장\n",
    "    word=li[0]\n",
    "    # 나머지를 실수로 변환해 vector 객체에 저장\n",
    "    vector=np.asarray(li[1:],dtype='float32')\n",
    "    # 키: word, 값: vector 로 하는 자료쌍을 사전에 저장\n",
    "    dictionary[word]=vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945cc42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 가까운 단어를 찾아주는 함수\n",
    "# vector와 거리가 가까운 순으로 정렬된 딕셔너리의 키를 반환\n",
    "def find_closest_words(vector):  # 100차원 벡터인 vector를 매개변수로 받음.\n",
    "    return sorted(dictionary.keys(), key=lambda w: distance.euclidean(dictionary[w],vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aea8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가까운 단어 5개 찾기\n",
    "# 자기 자신을 빼고 출력하려면 [1:5] 로 적용하면 됨.\n",
    "print(find_closest_words(dictionary['movie'])[:5])\n",
    "print(find_closest_words(dictionary['school'])[:5])\n",
    "print(find_closest_words(dictionary['oak'])[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 추론\n",
    "# 서울과 코리아의 관계는 마드리드와 스페인의 관계\n",
    "print(find_closest_words(dictionary[\"seoul\"]-dictionary[\"korea\"]+dictionary[\"spain\"])[:5])\n",
    "print(find_closest_words(dictionary[\"animal\"]-dictionary[\"lion\"]+dictionary[\"oak\"])[:5])\n",
    "print(find_closest_words(dictionary[\"queen\"]-dictionary[\"king\"]+dictionary[\"actress\"])[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어가 임베딩공간에 분포하는 영상을 시각화\n",
    "# tsne를 이용하여 2차원 공간으로 축소하고 시각화\n",
    "# TSNE : 원래 데이터분포를 최대한 유지하면서 저차원으로 변환 목적\n",
    "\n",
    "# n_components=2 : 2차원으로 축소하라는 뜻\n",
    "tsne=TSNE(n_components=2,random_state=0)\n",
    "words=list(dictionary.keys())\n",
    "vectors=[dictionary[word] for word in words]\n",
    "# 학습에 참여할 단어를 100개로 제한\n",
    "p2=tsne.fit_transform(vectors[:100])\n",
    "plt.scatter(p2[:,0],p2[:,1])\n",
    "\n",
    "for label,x,y in zip(words,p2[:,0],p2[:,1]):\n",
    "    plt.annotate(label,xy=(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492557c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
